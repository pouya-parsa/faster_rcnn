{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faster_rcnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ge4ZFmb0OGxP",
        "HBWpd4HIN0WD",
        "7om054x-N3xf",
        "1AQQn2bUagSy",
        "8gtPEwWobJ1V",
        "T7haPYHrMmbP",
        "PYki6lDX9aPY",
        "l80_KFsHwGWB",
        "FFtmE4lJ9nOg",
        "RQXJ7qyTBgnU",
        "qYpSvma60TJA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "308809f3caeb4fb38f35f9523e4f5617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f52bd836bb31418aa438e4c2af1d8ce4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05e323afc8ed42ba940cee5a8b7c8570",
              "IPY_MODEL_4fb975c2b001483eb6f1dca3f95e3156",
              "IPY_MODEL_d85f0fa02159453d87bc6e13c453fb02"
            ]
          }
        },
        "f52bd836bb31418aa438e4c2af1d8ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05e323afc8ed42ba940cee5a8b7c8570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c79bf6eb729d41ffa1b15231da2082cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e66f830e8314058b6a00eb0b5d0b630"
          }
        },
        "4fb975c2b001483eb6f1dca3f95e3156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4915be2399ee479a8a3f4532df469267",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c1e98a9dc57493ca1ee045790280b9c"
          }
        },
        "d85f0fa02159453d87bc6e13c453fb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2ef8850c91040a6a24a856511f5a993",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [00:35&lt;00:00, 70.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b00831431a8545fab5586f64c0d7289f"
          }
        },
        "c79bf6eb729d41ffa1b15231da2082cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e66f830e8314058b6a00eb0b5d0b630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4915be2399ee479a8a3f4532df469267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c1e98a9dc57493ca1ee045790280b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2ef8850c91040a6a24a856511f5a993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b00831431a8545fab5586f64c0d7289f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiH2vmxgCR-7",
        "outputId": "d75a2752-0fdf-486d-ff47-4a9136f801a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ziH2lPCcMn"
      },
      "source": [
        "!mkdir \"/content/vos\"\n",
        "!sudo unzip \"/content/drive/MyDrive/vos/train.zip\" -d \"/content/vos\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MmRVh-yeFQ8"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKnHnZnJb-rm",
        "outputId": "d6a128ee-5f49-4b58-fe18-084dd226253c"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 1\n",
        "pos_ratio = 0.5\n",
        "n_sample = 256\n",
        "n_pos = pos_ratio * n_sample\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdrDnvznfcxP"
      },
      "source": [
        "## Visualize Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eorEvi5ZJk7"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "colors = [(255, 128, 0), (255, 255, 0), (128, 255, 0), (0, 255, 0), (0, 255, 255), (0, 0, 255), (255, 0, 255), (96, 96, 96)]\n",
        "\n",
        "def visualize(image_path, anchor_boxes, gt_boxes=None):\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((800, 800))\n",
        "\n",
        "  image = np.array(image)\n",
        "  \n",
        "  if gt_boxes is not None:\n",
        "    for i_box, box in enumerate(gt_boxes):\n",
        "      \n",
        "      start_point = (int(box[0]), int(box[1]))\n",
        "      end_point = (int(box[2]), int(box[3]))\n",
        "      \n",
        "      cv2.rectangle(image, start_point, end_point, (255, 0, 0), 2)\n",
        "\n",
        "  for i_box, box in enumerate(anchor_boxes):\n",
        "    \n",
        "    start_point = (int(box[0]), int(box[1]))\n",
        "    end_point = (int(box[2]), int(box[3]))\n",
        "    color_index = i_box % len(colors) \n",
        "    cv2.rectangle(image, start_point, end_point, colors[color_index], 2)\n",
        "  \n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(image)\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVIIIRdHZ5mn"
      },
      "source": [
        "## UNIT TEST\n",
        "# valid_index = get_valid_anchor_boxes(anchor_boxes)\n",
        "# visualize(\"/content/image.jpg\", anchor_boxes[valid_index][1000:1005], get_bounding_box(\"/content/image.jpg\", \"/content/label.png\"))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKN8RGo3JO_M"
      },
      "source": [
        "def visualize_a_box(image_path, box):\n",
        "  visualize(image_path, [box])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBigJLTRfZGa"
      },
      "source": [
        "## Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuEx7jXOxo6h"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDshGdCQTg9w"
      },
      "source": [
        "def gen_anchor_box_for_single_feature_map(center_X, center_Y, scales, ratios):\n",
        "  \n",
        "  # Anchor Boxes are generated in the format (x1, y1, x2, y2)\n",
        "  \n",
        "  k = 0\n",
        "  boxes = np.zeros((len(scales) * len(ratios), 4))\n",
        "\n",
        "  for ratio in ratios:\n",
        "    \n",
        "    for scale in scales:\n",
        "\n",
        "      W = scale * subsample * ratio\n",
        "      H = scale * subsample\n",
        "\n",
        "      x_left_top = center_X - (1/2) * W\n",
        "      y_left_top = center_Y - (1/2) * H\n",
        "\n",
        "      x_right_bottom = center_X + (1/2) * W\n",
        "      y_right_bottom = center_Y + (1/2) * H \n",
        "\n",
        "      boxes[k, 0] = x_left_top\n",
        "      boxes[k, 1] = y_left_top\n",
        "      boxes[k, 2] = x_right_bottom\n",
        "      boxes[k, 3] = y_right_bottom\n",
        "\n",
        "      k += 1\n",
        "\n",
        "  return boxes\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Zo-FEJYZK7"
      },
      "source": [
        "## UNIT TEST\n",
        "# gen_anchor_box_for_single_feature_map(400, 400, [8, 16, 32], [0.5, 1, 2]) # output has to be in shape of (3 * 3, 4)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVYG4qOdKEnu"
      },
      "source": [
        "subsample = 16\n",
        "scales = [8, 16, 32]\n",
        "ratios = [0.5, 1, 2]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abpYUVzNOORt"
      },
      "source": [
        "def generate_all_anchor_boxes():\n",
        "  \n",
        "  anchor_boxes = np.zeros((50, 50, len(scales) * len(ratios), 4)) \n",
        "  \n",
        "  # center_X, center_Y\n",
        "  for i, center_X in enumerate(np.arange(8, 16 * (50), 16 )):\n",
        "    for j, center_Y in enumerate(np.arange(8, 16 * (50), 16)):\n",
        "      anchor_boxes[i, j] = gen_anchor_box_for_single_feature_map(center_X, center_Y, scales, ratios)\n",
        "\n",
        "  return anchor_boxes.reshape(-1, 4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhJD-BM2pir5"
      },
      "source": [
        "### Valid anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15QMn6JkpmdL"
      },
      "source": [
        "def get_valid_anchor_boxes(anchor_boxes):\n",
        "  # x1 and x2 have to in range (0, 800), same applies to y1, y2\n",
        "  inside_anchors_index = (anchor_boxes[:, 0] >= 0) & (anchor_boxes[:, 2] < 800) &\\\n",
        "  (anchor_boxes[:, 1] >= 0) & (anchor_boxes[:, 3] < 800)\n",
        "\n",
        "  # valid_anchor_boxes = anchor_boxes[inside_anchors_index]\n",
        "\n",
        "  return inside_anchors_index"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzQKynOqh-N"
      },
      "source": [
        "# UNIT TEST\n",
        "# the exact number depends on the scales and ratios you used\n",
        "# (inside_anchors_index == True).sum() > 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge4ZFmb0OGxP"
      },
      "source": [
        "## Compute IoU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8zITOdTaA2g"
      },
      "source": [
        "def compute_iou(box1, box2):\n",
        "  # assumeing boxes are in format (x1, y1, x2, y2)\n",
        "  inter = {}\n",
        "  inter[\"x_left_top\"] = max(box1[0], box2[0])\n",
        "  inter[\"y_left_top\"] = max(box1[1], box2[1])\n",
        "\n",
        "  inter[\"x_right_bottom\"] = min(box1[2], box2[2])\n",
        "  inter[\"y_right_bottom\"] = min(box1[3], box2[3])\n",
        "\n",
        "  if inter[\"x_left_top\"] < inter[\"x_right_bottom\"] and inter[\"y_left_top\"] < inter[\"y_right_bottom\"]: # there is a non-zero intersection  \n",
        "    iou_area = (inter[\"x_right_bottom\"] - inter[\"x_left_top\"]) * (inter[\"y_right_bottom\"] - inter[\"y_left_top\"])\n",
        "  else:\n",
        "    iou_area = 0\n",
        "\n",
        "  box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "  box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "  iou = iou_area / (box1_area + box2_area - iou_area)\n",
        "\n",
        "  return iou"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTbbGaX6vNpq"
      },
      "source": [
        "## UNIT TEST\n",
        "# box1 = [0, 0, 5, 5]\n",
        "# box2 = [5, 5, 10, 10]\n",
        "# box3 = [2.5, 2.5, 7.5, 7.5]\n",
        "\n",
        "# compute_iou(box1, box2) # it has to be zero\n",
        "# compute_iou(box1, box3) # ~ 0.14\n",
        "# compute_iou(box3, box1) # it's a symetric function so same 0.14"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBWpd4HIN0WD"
      },
      "source": [
        "## Get Bouning Box from Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OhiuzvtrWLY"
      },
      "source": [
        "def get_number_of_instances(mask_path):\n",
        "  \n",
        "  mask = Image.open(mask_path)\n",
        "  mask = np.array(mask)\n",
        "\n",
        "  return len(np.unique(mask))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gvTsO2TG1V8"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_bounding_box(mask_path):\n",
        "\n",
        "  mask = Image.open(mask_path)\n",
        "  \n",
        "  mask = mask.resize((800, 800))  # otherwise boxes don't place at right locations \n",
        "\n",
        "  mask = np.array(mask)\n",
        "\n",
        "  # mask is in shape of [H, W] where each pixel has a value of 0, 1, 2, 3, 4\n",
        "  # we don't care about different categories, we care just about being an object\n",
        "\n",
        "  boxes = []\n",
        "\n",
        "  for instance_number in np.unique(mask):\n",
        "    if instance_number == 0: \n",
        "      continue\n",
        "\n",
        "    Y, X = np.where(mask == instance_number)\n",
        "    box = [X.min(), Y.min(), X.max(), Y.max()]\n",
        "    boxes.append(box)\n",
        "  \n",
        "  return boxes"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlqnhKBaMsl6"
      },
      "source": [
        "# UNIT TEST\n",
        "# boxes = get_bounding_box(\"/content/vos/train/JPEGImages/2d8f5e5025/00055.jpg\", \"/content/vos/train/Annotations/2d8f5e5025/00055.png\")\n",
        "# visualize(\"/content/vos/train/JPEGImages/10b31f5431/00030.jpg\", boxes)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7om054x-N3xf"
      },
      "source": [
        "## Compute (50 * 50 * 9, 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf4SGKpQc_-J"
      },
      "source": [
        "def get_anchors_iou(image_path, label_path, anchors):\n",
        "  \n",
        "  gt_boxes = get_bounding_box(label_path)\n",
        "\n",
        "  anchors_iou = np.zeros((len(anchors), len(gt_boxes)))\n",
        "\n",
        "  for anchor_ind, anchor_box in enumerate(anchors):\n",
        "    for gt_idx, gt_box in enumerate(gt_boxes):\n",
        "      anchors_iou[anchor_ind, gt_idx] = compute_iou(anchor_box, gt_box)\n",
        "  \n",
        "  return anchors_iou    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmxMGEqhPU9k"
      },
      "source": [
        "def iou_to_label(anchors_iou):\n",
        "\n",
        "  # anchors_iou is the mutual iou of every anchor box and ground-truth box\n",
        "  # is in shape of (50 * 50 * 9, 5)\n",
        "\n",
        "  # output is in shape of (50 * 50 * 9, 1)\n",
        "\n",
        "  \"\"\"\n",
        "  a) if iou of anchor box and ground-truth box is greater than 0.7 label one is assigned\n",
        "  b) anchor box with highest iou with a specific ground-truth box is also labeled as one\n",
        "  c) iou less than 0.3 is labeled as -1\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  anchor_label = np.full((anchors_iou.shape[0], 1), fill_value=-1)\n",
        "\n",
        "\n",
        "  negative_a_idx = np.where(np.max(anchors_iou, axis=-1) < 0.3) # a rule\n",
        "  \n",
        "  positive_b_idx = np.where(np.max(anchors_iou, axis=-1) > 0.7) # b rule\n",
        "  postive_c_idx =  np.argmax(anchors_iou, axis=0) # c rule\n",
        "  \n",
        "  anchor_label[negative_a_idx, :] = 0\n",
        "\n",
        "  anchor_label[positive_b_idx, :] = 1\n",
        "  anchor_label[postive_c_idx, :] = 1\n",
        "  \n",
        "  \n",
        "  \n",
        "  return anchor_label"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GNXRcjYXKgR"
      },
      "source": [
        "# UNIT TEST\n",
        "# anchors_iou = get_anchors_iou(\"/content/image.jpg\", \"/content/label.png\", anchor_boxes)\n",
        "# anchor_label = iou_to_label(anchors_iou)\n",
        "# X1, _ = np.where(anchor_label == 1)\n",
        "# visualize(\"/content/image.jpg\", anchor_boxes[X1])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7jsq8Oz1QNn"
      },
      "source": [
        "# SANITY CHECK\n",
        "# anchors with label zero should be much higher in number than label 1 and -1\n",
        "# label 1 reasonable range is (grouth_truth, 3 * grouth_truth)\n",
        "\n",
        "# (anchor_label == -1).sum(), (anchor_label == 0).sum(), (anchor_label == 1).sum() "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AQQn2bUagSy"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aqK047McpeZ"
      },
      "source": [
        "def corner2center(box):\n",
        "\n",
        "  x1 = box[0]\n",
        "  x2 = box[2]\n",
        "\n",
        "  y1 =  box[1]\n",
        "  y2 = box[3]\n",
        "\n",
        "  center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "  width, height = (x2 - x1), (y2 - y1)\n",
        "\n",
        "  return [center_x, center_y, width, height]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiUA610LsoUL"
      },
      "source": [
        "# UNIT TEST\n",
        "# box1 = corner2center([0, 0, 5, 5]) \n",
        "# assert box1 == [2.5, 2.5, 5, 5]\n",
        "# print(\"pass!\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIKf8_wSHdxB"
      },
      "source": [
        "def center2corner(box):\n",
        "  x, y, w, h = box\n",
        "  x1 = x - (1/2) * w\n",
        "  y1 = y - (1/2) * h\n",
        "  x2 = x + (1/2) * w\n",
        "  y2 = y + (1/2) * h\n",
        "\n",
        "  return [x1, y1, x2, y2]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfhaTwsIfG1X"
      },
      "source": [
        "# UNIT TEST\n",
        "# box1 = center2corner([2.5, 2.5, 5, 5]) \n",
        "# assert box1 == [0, 0, 5, 5]\n",
        "# print(\"pass!\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps-OV-DOt54Z"
      },
      "source": [
        "$$ t_x = \\frac{A_x - G_x}{A_w} $$\n",
        "\\\n",
        "$$ t_w = \\log{\\frac{G_w}{A_w}} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ONJ1wPdG_l"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def get_parameterized_target(anchor_box, gt_box):\n",
        "  \n",
        "  # input is anchor_box, gt_box in center format\n",
        "  # output is t_X, t_y, t_h, t_w\n",
        "  A_x, A_y, A_w, A_h = anchor_box\n",
        "  G_x, G_y, G_w, G_h = gt_box\n",
        "\n",
        "  t_x = (A_x - G_x) / A_w\n",
        "  t_y = (A_y - G_y) / A_h\n",
        "  t_w = log(G_w / A_w)\n",
        "  t_h = log(G_h / A_h)\n",
        "\n",
        "  return [t_x, t_y, t_w, t_h] "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LabosgWeCJ9"
      },
      "source": [
        "# UNIT TEST\n",
        "# box1 = corner2center([0., 0., 5., 5.])\n",
        "# box2 = corner2center([2.5, 2.5, 7.5, 7.5])\n",
        "\n",
        "# assert (get_parameterized_target(box1, box2) == [-1/2, -1/2, 0, 0]) # expected output [-1/2, -1/2, 0, 0]\n",
        "# print(\"pass!\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIpCuNVytn7l"
      },
      "source": [
        "def parametrized_to_corner(t_x, t_y, t_w, t_h, anchor_box):\n",
        "  # anchor_box in format center\n",
        "  A_x, A_y, A_w, A_h = anchor_box\n",
        "  \n",
        "  pred_x = A_x - t_x * A_w  \n",
        "  pred_y = A_y - t_y * A_h\n",
        "\n",
        "  pred_w = np.exp(t_w) * A_w  \n",
        "  pred_h = np.exp(t_h) * A_h\n",
        "\n",
        "  return center2corner([pred_x, pred_y, pred_w, pred_h])  "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq0h9DQhtgK9"
      },
      "source": [
        "# UNIT TEST\n",
        "# box1 = corner2center([0., 0., 5., 5.])\n",
        "# box2 = corner2center([2.5, 2.5, 7.5, 7.5])\n",
        "\n",
        "# assert (parametrized_to_corner(*[-1/2, -1/2, 0, 0], box1) == center2corner(box2))\n",
        "# print(\"pass!\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUVyPrl7v0YW"
      },
      "source": [
        "## RPN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZj9JIaUwb3w"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRMdlbjaBqxs"
      },
      "source": [
        "feature_extractor = vgg16(pretrained=True).features[:30].to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6LYekcEwRGU"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "  \n",
        "  def __init__(self, n_anchors):\n",
        "    \n",
        "    super(RPN, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "    self.conv1.weight.data.normal_(0, 0.01)\n",
        "    self.conv1.bias.data.zero_()\n",
        "\n",
        "    self.cls_layer = nn.Conv2d(512, n_anchors * 2, 1, 1, 0)\n",
        "    self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "    self.cls_layer.bias.data.zero_()\n",
        "\n",
        "    self.reg_layer = nn.Conv2d(512, n_anchors * 4, 1, 1, 0)\n",
        "    self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "    self.cls_layer.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "\n",
        "    pred_cls = self.cls_layer(x)\n",
        "    pred_loc = self.reg_layer(x)\n",
        "\n",
        "    return pred_cls, pred_loc"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vulkuystU9Ry"
      },
      "source": [
        "class Faster_RCNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    \n",
        "    super(Faster_RCNN, self).__init__()\n",
        "    n_anchors = 9\n",
        "    self.rpn_model = RPN(n_anchors).to(device)\n",
        "\n",
        "    self.adaptive_max_pool = nn.AdaptiveMaxPool2d((7, 7))\n",
        "\n",
        "    self.roi_head_classifier = nn.Sequential(\n",
        "      nn.Linear(25088, 4096),\n",
        "      nn.Linear(4096, 4096)\n",
        "    )\n",
        "\n",
        "    self.score = nn.Linear(4096, 21)\n",
        "    self.cls_loc = nn.Linear(4096, 21 * 4)\n",
        "\n",
        "  def rpn(self, x):\n",
        "    return self.rpn_model(x)\n",
        "\n",
        "  def forward(self, x, indices_and_rois):\n",
        "    \n",
        "    output = []\n",
        "\n",
        "    for idx, roi in enumerate(indices_and_rois):\n",
        "      \n",
        "      image_index, x1, y1, x2, y2 = roi\n",
        "\n",
        "      x1, y1, x2, y2 = x1 // 16, y1 // 16, x2 // 16, y2 // 16\n",
        "      im = x.narrow(0, 0, 1)[..., y1:y2+1, x1:x2+1]\n",
        "\n",
        "      output.append(self.adaptive_max_pool(im)) # (512, 7, 7)\n",
        "\n",
        "    output = torch.cat(output, 0)\n",
        "    output = output.view(128, -1) # (128, 25088)25088\n",
        "\n",
        "    print(output.shape)\n",
        "    \n",
        "    output = self.roi_head_classifier(output)\n",
        "    pred_roi_cls_score = self.score(output)\n",
        "    pred_roi_loc = self.cls_loc(output)\n",
        "\n",
        "    return pred_roi_cls_score, pred_roi_loc "
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18lOMebw5Nw"
      },
      "source": [
        "# n_anchors = len(scales) * len(ratios)\n",
        "# n_anchors = 9\n",
        "# rpn = RPN(n_anchors).to(device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dscff0IzsdN"
      },
      "source": [
        "# UNIT Test\n",
        "# assert (rpn.feature_extractor(torch.randn(4, 3, 800, 800)).shape) == (4, 512, 50, 50), \"something wrong with feature extractor output shape!\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtPEwWobJ1V"
      },
      "source": [
        "## NMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMqETyd7Mj2U"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def NMS(boxes):\n",
        "  # boxes in order of the objectness score\n",
        "  boxes_detected = []\n",
        "  boxes_remained = np.copy(boxes)\n",
        "\n",
        "  while len(boxes_remained) > 0:\n",
        "    \n",
        "    if len(boxes_detected) >= 2000:\n",
        "      return boxes_detected[:2000]\n",
        "\n",
        "    box_detected = boxes_remained[0]\n",
        "    boxes_detected.append(box_detected)\n",
        "    \n",
        "    boxes_remained = boxes_remained[1:]\n",
        "\n",
        "    mask = np.ones(len(boxes_remained), dtype=bool)\n",
        "    idx_to_remove = []\n",
        "    \n",
        "    for idx, box in enumerate(boxes_remained):\n",
        "      \n",
        "      if compute_iou(box_detected, box) > 0.7:\n",
        "        idx_to_remove.append(idx)\n",
        "    \n",
        "    boxes_remained = boxes_remained[mask, ...]\n",
        "  return boxes_detected[:2000]\n",
        "  "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7haPYHrMmbP"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEdWuzYoNeWh"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlGW7bCxztn2"
      },
      "source": [
        "anchor_boxes = generate_all_anchor_boxes()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk7oV7fNNjjO"
      },
      "source": [
        "class VOS(Dataset):\n",
        "\n",
        "  def __init__(self, root):\n",
        "    self.images = np.array(glob.glob(os.path.join(root, \"JPEGImages/*/*.jpg\")))[:2000]\n",
        "    self.root = root\n",
        "\n",
        "    idx_to_keep = []\n",
        "    print(\"Removing images with no GT-box\")\n",
        "    for idx, image_path in enumerate(tqdm(self.images)):\n",
        "      \n",
        "      image_name = image_path.split(\"/\")[-1].split(\".\")[0]\n",
        "      folder_id = image_path.split(\"/\")[-2]\n",
        "      label_path = os.path.join(self.root, \"Annotations\", folder_id, image_name + \".png\")\n",
        "      \n",
        "      number_of_instances = get_number_of_instances(label_path)\n",
        "      if number_of_instances > 1:\n",
        "        idx_to_keep.append(idx)\n",
        "\n",
        "    \n",
        "    self.images = self.images[idx_to_keep]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    image_path = self.images[idx]\n",
        "    \n",
        "    image_name = image_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    folder_id = image_path.split(\"/\")[-2]\n",
        "    label_path = os.path.join(self.root, \"Annotations\", folder_id, image_name + \".png\")\n",
        "\n",
        "\n",
        "    iou = get_anchors_iou(image_path, label_path, anchor_boxes)\n",
        "\n",
        "    highest_iou = np.argmax(iou, axis=-1) # only GT-box with highest IoU is considered\n",
        "    gt_boxes = get_bounding_box(label_path)\n",
        "\n",
        "    # for each anchor box we have (t_x, t_y, t_w, t_h)\n",
        "    # ground truth box with highest iou is chosen\n",
        "    loc = np.zeros((len(anchor_boxes), 4)) \n",
        "    label = iou_to_label(iou)\n",
        "    \n",
        "    # ignore anchor boxes outside the image\n",
        "    valid_indexes = get_valid_anchor_boxes(anchor_boxes)\n",
        "    label[~valid_indexes] = -1\n",
        "\n",
        "    # sampling positive and negative anchor boxes with ratio 1:1 (if possible)\n",
        "    pos_index = np.where(label == 1)[0]\n",
        "    disable = np.random.choice(pos_index, size=max(0, len(pos_index) - 128), replace=False)\n",
        "    label[disable] = -1\n",
        "\n",
        "    neg_index = np.where(label == 0)[0]\n",
        "    pos_count = np.sum(label == 1)\n",
        "    disable = np.random.choice(neg_index, size=len(neg_index) - (n_sample - pos_count), replace=False)\n",
        "    label[disable] = -1\n",
        "\n",
        "\n",
        "    for anchor_idx in range((len(anchor_boxes))):\n",
        "      \n",
        "      anchor_center = corner2center(anchor_boxes[anchor_idx]) \n",
        "      gt_center = corner2center(gt_boxes[highest_iou[anchor_idx]]) # coordination of nearest GT-box\n",
        "\n",
        "      loc[anchor_idx] = get_parameterized_target(anchor_center, gt_center)\n",
        "\n",
        "   \n",
        "    image_file = Image.open(image_path)\n",
        "    image_file = image_file.resize((800, 800))\n",
        "    image_tensor = transforms.ToTensor()((np.array(image_file)))\n",
        "\n",
        "    return image_path, label_path, image_tensor, label, loc"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYhBjrIlwBjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "308809f3caeb4fb38f35f9523e4f5617",
            "f52bd836bb31418aa438e4c2af1d8ce4",
            "05e323afc8ed42ba940cee5a8b7c8570",
            "4fb975c2b001483eb6f1dca3f95e3156",
            "d85f0fa02159453d87bc6e13c453fb02",
            "c79bf6eb729d41ffa1b15231da2082cf",
            "5e66f830e8314058b6a00eb0b5d0b630",
            "4915be2399ee479a8a3f4532df469267",
            "8c1e98a9dc57493ca1ee045790280b9c",
            "d2ef8850c91040a6a24a856511f5a993",
            "b00831431a8545fab5586f64c0d7289f"
          ]
        },
        "outputId": "7614e4b4-0394-4ed6-c72d-f84589a89366"
      },
      "source": [
        "vos = VOS(\"/content/vos/train\")\n",
        "train_loader = DataLoader(vos, batch_size=batch_size, shuffle=True)\n",
        "len(train_loader)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing images with no GT-box\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308809f3caeb4fb38f35f9523e4f5617",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1994"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5XFb6L7Pg00"
      },
      "source": [
        "# vos = VOS(\"/content/vos/train\")\n",
        "# image_file, label, loc = vos[0]\n",
        "\n",
        "# # Sanity Check\n",
        "# image_file.shape, label.shape, loc.shape\n",
        "# print((label == 0).sum())\n",
        "# print((label == 1).sum())\n",
        "\n",
        "# # UNIT TEST\n",
        "# (label == 0).sum() + (label == 1).sum() == n_sample"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9f4D1TfhQX4"
      },
      "source": [
        "# image_tensor, label, loc = next(iter(train_loader))\n",
        "# image_tensor.shape, label.shape, loc.shape"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYki6lDX9aPY"
      },
      "source": [
        "## Post Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x89bN6i4ZZo"
      },
      "source": [
        "$$ t_x = \\frac{A_x - G_x}{A_w} $$\n",
        "\\\n",
        "$$ t_w = \\log{\\frac{G_w}{A_w}} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUhgGX9CTUI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l4bO2c23HmU"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def get_pred_loc_corner(pred_loc):\n",
        "\n",
        "  pred_loc_numpy = pred_loc.detach().cpu().numpy()\n",
        "  pred_location_convert = np.zeros_like(pred_loc_numpy)\n",
        "\n",
        "\n",
        "  for idx, pred in enumerate(pred_loc_numpy):\n",
        "    A_x, A_y, A_w, A_h = corner2center(anchor_boxes[idx])\n",
        "    t_x, t_y, t_w, t_h = pred\n",
        "\n",
        "    x1, y1, x2, y2 = parametrized_to_corner(t_x, t_y, t_w, t_h, [A_x, A_y, A_w, A_h])\n",
        "\n",
        "    pred_location_convert[idx] = [x1, y1, x2, y2]\n",
        "  \n",
        "\n",
        "  return pred_location_convert\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq-dR1-bRM41"
      },
      "source": [
        "# UNIT TEST\n",
        "# box1 = corner2center([0., 0., 5., 5.])\n",
        "# box2 = corner2center([2.5, 2.5, 7.5, 7.5])\n",
        "# (get_pred_loc_corner(torch.tensor([[-1/2, -1/2, 0, 0]])))\n",
        "# assert (get_pred_loc_corner(torch.tensor([[-1/2, -1/2, 0, 0]]))) == [[  8.,   8.,  72., 136.]]\n",
        "# print(\"pass!\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l80_KFsHwGWB"
      },
      "source": [
        "## Filter By Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A7MZm_qmJd3"
      },
      "source": [
        "def filter_by_size(pred_loc):\n",
        "  # pred loc is numpy array (22500, 4)\n",
        "  \n",
        "  filtered_indexes =( (pred_loc[:, 2] - pred_loc[:, 0]) > 16) & ((pred_loc[:, 3] - pred_loc[:, 1]) > 16)\n",
        "  \n",
        "  return pred_loc[filtered_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvhWLGDoml8M"
      },
      "source": [
        "# UNIT TEST\n",
        "# filter_by_size(np.random.rand(22500, 4) * 100).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFtmE4lJ9nOg"
      },
      "source": [
        "## Clip to the Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKILjat99o9M"
      },
      "source": [
        "def clip_into_image(boxes):\n",
        "  \n",
        "  boxes[:, 0] = np.clip(boxes[:, 0], 0 + 5, 800-5)\n",
        "  boxes[:, 1] = np.clip(boxes[:, 1], 0 + 5, 800-5)\n",
        "  boxes[:, 2] = np.clip(boxes[:, 2], 0 + 5, 800-5)\n",
        "  boxes[:, 3] = np.clip(boxes[:, 2], 0 + 5, 800-5)\n",
        "\n",
        "  return boxes"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlljhfS6Q0bG",
        "outputId": "edc84d29-92b9-4343-baa1-e37cbc030901"
      },
      "source": [
        "# UNIT TEST\n",
        "boxes = torch.randn((2000, 4))\n",
        "clip_into_image(boxes).shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2000, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQXJ7qyTBgnU"
      },
      "source": [
        "## Sort Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIgU-zQbBh0J"
      },
      "source": [
        "def get_high_score(objectness_score, pred_loc):\n",
        "\n",
        "  objectness_score = objectness_score.detach().cpu()\n",
        "  order = objectness_score.ravel().argsort(descending=True)\n",
        "  pred_ordered = pred_loc[order[:12000], :]\n",
        "  \n",
        "  return pred_ordered"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uv2t1H1Qev6"
      },
      "source": [
        "# UNIT TEST\n",
        "# pred_loc = torch.randn((22500, 4))\n",
        "# objectness_score = torch.randn(len(pred_loc))\n",
        "\n",
        "# get_high_score(objectness_score, pred_loc).shape"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpSvma60TJA"
      },
      "source": [
        "## Sampling ROIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbOKxt22-1n7"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qqdUJzTkmbW"
      },
      "source": [
        "def get_roi_sample(image_path, label_path, ROIs):\n",
        "  \n",
        "  anchors_iou = get_anchors_iou(image_path, label_path, ROIs) # (2000, 5)\n",
        "  \n",
        "  max_iou = np.max(anchors_iou, axis=-1)\n",
        "  \n",
        "  gt_index = np.argmax(anchors_iou, axis=-1) # (2000, 1)\n",
        "\n",
        "  pos_labels = np.where(max_iou > 0.5)[0]\n",
        "\n",
        "  neg_labels = np.where((max_iou > 0.1) & (max_iou < 0.5))[0]\n",
        "\n",
        "  positive_labels = np.random.choice(pos_labels, size=min(len(pos_labels), 32), replace=False)\n",
        "  negative_labels = np.random.choice(neg_labels, size=min(len(neg_labels), 96), replace=False)\n",
        "\n",
        "  sample_index = np.append(positive_labels, negative_labels)\n",
        "  labels = np.zeros(128)\n",
        "  labels[:32] = 1\n",
        "\n",
        "  return labels, ROIs[sample_index], gt_index[sample_index]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnAC32SmPj4_"
      },
      "source": [
        "# UNIT TEST \n",
        "# get_roi_sample(\"/content/image.jpg\", \"/content/label.png\", torch.randn(2000, 4))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-tJngL8EZIE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qrrm9ggTSXG"
      },
      "source": [
        "faster_rcnn = Faster_RCNN().to(device)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xosVwz3fXMog"
      },
      "source": [
        "faster_rcnn_optim = torch.optim.Adam(faster_rcnn.parameters(), lr=0.001)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuWQkg4tMoR7"
      },
      "source": [
        "regression_loss_acc = []\n",
        "classification_loss_acc = []\n",
        "\n",
        "for iteration, (image_path, mask_path, image, cls, loc)  in enumerate(tqdm(train_loader)):\n",
        "  # image in shape of (1, 3, 800, 800)\n",
        "  # cls in shape of (1, 22500, 1)\n",
        "  # loc in shape of (1, 22500, 4)\n",
        "  image_path = image_path[0]\n",
        "  mask_path = mask_path[0]\n",
        "\n",
        "\n",
        "  cls = cls[0].reshape(-1).to(device)\n",
        "  loc = loc[0].to(device)\n",
        "  image = image.to(device)\n",
        "  output_map = feature_extractor(image).detach()\n",
        "\n",
        "  faster_rcnn_optim.zero_grad()\n",
        "\n",
        "  pred_cls, pred_loc = faster_rcnn.rpn(output_map)\n",
        "\n",
        "  # pre-process for faster_rcnn\n",
        "  # pred_cls, pred_loc = rpn(output_map) # output should be (1, 9 * 2, 50 , 50), (1, 9 * 4, 50, 50)\n",
        "\n",
        "  pred_loc = pred_loc.contiguous().view(-1, 4)\n",
        "  # pred_cls = pred_cls.permute(0, 2, 3, 1).contiguous().view(1, 50, 50, 18)\n",
        "  objectness_score = pred_cls.view(50 * 50, 9, 2)[:, :, 1].contiguous().view(-1, 1)\n",
        "  pred_cls = pred_cls.view(-1, 2)\n",
        "\n",
        "  pred_loc_corner = get_pred_loc_corner(pred_loc) # (22500, 4) corner format\n",
        "  pred_loc_corner_filtered = filter_by_size(pred_loc_corner)\n",
        "  pred_loc_corner = clip_into_image(pred_loc_corner_filtered) # clip boxes into the height and width of image\n",
        "  pred_high_loc_corner = get_high_score(objectness_score, pred_loc_corner) # (12000, 4)\n",
        "  # pred_nms_loc_corner = NMS(pred_high_loc_corner) # (2000, 4)\n",
        "  roi_labels, roi_loc, gt_index = get_roi_sample(image_path, mask_path, pred_high_loc_corner) # (128, 1) (128, 4)\n",
        "\n",
        "  image_index = torch.zeros((len(roi_loc), 1))\n",
        "  roi_loc = torch.from_numpy(roi_loc)\n",
        "  indices_and_rois = torch.cat((image_index, roi_loc), dim=1)\n",
        "\n",
        "  gt_boxes = get_bounding_box(mask_path)\n",
        "  roi_loc_parametrized = np.zeros_like(roi_loc)\n",
        "\n",
        "  for idx, roi in enumerate(roi_loc):\n",
        "    gt_box = gt_boxes[gt_index[idx]]  \n",
        "    roi_loc_parametrized[idx] = get_parameterized_target(roi, gt_box)\n",
        "\n",
        "  pred_roi_cls_score, pred_roi_loc = faster_rcnn(output_map, indices_and_rois.long())\n",
        "\n",
        "  classification_loss = F.cross_entropy(pred_cls, cls, ignore_index=-1)\n",
        "  \n",
        "  # classification_loss_acc.append(classification_loss.item())\n",
        "  print(classification_loss.item())\n",
        "\n",
        "  pos_index = torch.where(cls == 1)\n",
        "  x = torch.abs(pred_loc[pos_index] - loc[pos_index])\n",
        "\n",
        "  regression_loss = (x < 1).float() * (0.5) * (x**2) + (x > 1).float() * (x - 0.5)\n",
        "  regression_loss = regression_loss.sum()\n",
        "  regression_loss = regression_loss / (len(pos_index))\n",
        "\n",
        "  regression_loss_acc.append(regression_loss.item())\n",
        "  print(regression_loss.item())\n",
        "\n",
        "  rpn_loss = classification_loss + (10 * regression_loss)\n",
        "\n",
        "  roi_labels, roi_loc_parametrized = torch.tensor(roi_labels).long().to(device), torch.tensor(roi_loc_parametrized).to(device)\n",
        "\n",
        "  ### Faster R-CNN\n",
        "  roi_cls_loss = F.cross_entropy(pred_roi_cls_score, roi_labels, ignore_index=-1)\n",
        "\n",
        "  pred_roi_loc = pred_roi_loc.view(-1, 21, 4)\n",
        "  pred_roi_loc = pred_roi_loc[torch.arange(0, 128).long(), 1] # (128, 4)\n",
        "\n",
        "  # pred_loc = pred_loc[torch.arange(0, 128).long(), 1]\n",
        "\n",
        "  x = torch.abs(pred_roi_loc - roi_loc_parametrized)\n",
        "  roi_reg_loss = (x < 1).float() * 0.5 * (x**2) + (x >= 1).float() * (x - 0.5)\n",
        "  roi_reg_loss = roi_reg_loss.sum() / len(roi_loc_parametrized.shape[0])\n",
        "  \n",
        "  faster_rcnn_loss = roi_cls_loss + (10 * roi_reg_loss) \n",
        "  \n",
        "  loss = rpn_loss + faster_rcnn_loss\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  faster_rcnn_optim.step()\n",
        "\n",
        "  if iteration % 100 == 0:\n",
        "    \n",
        "    # visualize 100 high score box\n",
        "    obj_score = pred_cls[:, 1] # [22500, 2]\n",
        "    \n",
        "    order_index = obj_score.argsort(dim=0)[:12000]\n",
        "    predicted_boxes = pred_loc[order_index].detach().cpu().numpy()\n",
        "    for idx, pred_box in enumerate(predicted_boxes):\n",
        "      anchor_box = anchor_boxes[order_index[idx]]\n",
        "      predicted_boxes[idx] = parametrized_to_corner(*pred_box, anchor_box)\n",
        "\n",
        "    predicted_boxes = clip_into_image(predicted_boxes)\n",
        "    detected_boxes = NMS(predicted_boxes) #[22500, 4]\n",
        "    print(len(detected_boxes))\n",
        "    # print(image_path)\n",
        "    visualize(image_path[0], detected_boxes, get_bounding_box(mask_path[0]))\n",
        "\n",
        "    print(\"classification loss: \", np.mean(classification_loss_acc[-100:]))\n",
        "    print(\"regression loss:\", np.mean(regression_loss_acc[-100:]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}